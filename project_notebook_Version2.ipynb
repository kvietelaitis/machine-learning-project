{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human Activity Recognition — Course Project\n",
        "\n",
        "Authors: girdauskaite + partner\n",
        "\n",
        "Notebook skeleton to run the full project: data loading, cleaning, EDA, baseline models, required models (RandomForest, XGBoost), additional methods (CatBoost, k-NN), hyperparameter tuning, ablation experiments, and interpretation (SHAP).\n",
        "\n",
        "Run cells sequentially. Edit paths and parameters where noted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Environment / Install packages (run once)\n",
        "\n",
        "If you are running in a clean environment, run the following cell to install missing packages. In many hosted environments (Colab, Kaggle) some packages are preinstalled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install xgboost lightgbm catboost shap tensorflow scikit-learn seaborn matplotlib\n",
        "# Uncomment the above line and run if packages are missing in your env."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('imports done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Config / Paths / Label mapping\n",
        "Update paths if your CSVs are in a different location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "TRAIN_CSV = '/data/activity_train.csv'  # change if needed\n",
        "TEST_CSV = '/data/activity_test.csv'    # if provided; may not exist\n",
        "\n",
        "# Label mapping based on assignment (confirm with instructor files if different)\n",
        "label_map = {\n",
        "    1:'WALKING', 2:'WALKING_UPSTAIRS', 3:'WALKING_DOWNSTAIRS',\n",
        "    4:'SITTING', 5:'STANDING', 6:'LAYING',\n",
        "    7:'STAND_TO_SIT', 8:'SIT_TO_STAND', 9:'SIT_TO_LIE',\n",
        "    10:'LIE_TO_SIT', 11:'STAND_TO_LIE', 12:'LIE_TO_STAND'\n",
        "}\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "print('config set')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load data (train CSV)\n",
        "Quick sanity checks: shape, columns, head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(TRAIN_CSV)\n",
        "df_test  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "print('train shape:', df_train.shape)\n",
        "print('test shape: ', df_test.shape)\n",
        "\n",
        "# quick sanity: columns must match (except maybe target column)\n",
        "train_cols = list(df_train.columns)\n",
        "test_cols = list(df_test.columns)\n",
        "print('train cols count:', len(train_cols), 'test cols count:', len(test_cols))\n",
        "\n",
        "# If test has activity column name different or missing, handle below\n",
        "print('train columns sample:', train_cols[:10])\n",
        "print('test columns sample :', test_cols[:10])\n",
        "print('shape:', df.shape)\n",
        "display(df.columns[:120])\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Check for duplicate column names / typos\n",
        "We noticed some columns like `tBodyAcc_ropy` or duplicates in header — inspect duplicates and rename or deduplicate as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# list duplicated column names\n",
        "cols = pd.Series(df.columns)\n",
        "dups = cols[cols.duplicated()].unique()\n",
        "print('duplicates:', dups)\n",
        "\n",
        "if len(dups) > 0:\n",
        "    for d in dups:\n",
        "        print('\\n---', d)\n",
        "        print(df.loc[:, df.columns == d].iloc[:2, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If duplicates are harmless copies, you can drop identical columns; if they correspond to distinct features with slightly different names (typo), rename them to unique names. Example renaming is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: rename a mis-typed column if present (edit names as required)\n",
        "rename_map = {}\n",
        "# rename_map['tBodyAcc_ropy_1.1'] = 'tBodyAcc_ropy_1_copy'   # example\n",
        "if rename_map:\n",
        "    df.rename(columns=rename_map, inplace=True)\n",
        "    print('renamed columns')\n",
        "else:\n",
        "    print('no renames applied')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Quick target check and mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'activity' not in df.columns:\n",
        "    raise ValueError('No `activity` column found in CSV — check your CSV')\n",
        "\n",
        "print(df['activity'].nunique(), 'unique labels')\n",
        "print(df['activity'].value_counts().sort_index())\n",
        "\n",
        "df['activity_name'] = df['activity'].map(label_map)\n",
        "display(df['activity_name'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Basic EDA: class balance, some feature distributions, correlation heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "sns.countplot(y='activity_name', data=df, order=df['activity_name'].value_counts().index)\n",
        "plt.title('Class distribution (train csv)')\n",
        "plt.show()\n",
        "\n",
        "# display distribution for a few example features\n",
        "sample_features = list(df.columns[:10])\n",
        "df[sample_features].hist(bins=30, figsize=(12,8))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# correlation heatmap for the first 50 numeric features (visual check)\n",
        "numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "corr_cols = numeric[:50]\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df[corr_cols].corr(), cmap='coolwarm', center=0)\n",
        "plt.title('Correlation (first 50 numeric features)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Preprocessing: prepare X, y and train/test split\n",
        "If you have an official test CSV, load it. Otherwise create a stratified train/test split (70/30 as assignment suggests)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Features: drop target and name columns\n",
        "X = df.drop(columns=['activity', 'activity_name'])\n",
        "y = df['activity']\n",
        "\n",
        "print('X shape', X.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "print('train shape', X_train.shape, 'test shape', X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Check for NaNs / constant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "na_cols = X_train.columns[X_train.isna().any()].tolist()\n",
        "print('columns with NaN:', na_cols)\n",
        "\n",
        "# drop constant columns\n",
        "nunique = X_train.nunique()\n",
        "const_cols = nunique[nunique <= 1].index.tolist()\n",
        "print('constant columns:', const_cols)\n",
        "\n",
        "if const_cols:\n",
        "    X_train.drop(columns=const_cols, inplace=True)\n",
        "    X_test.drop(columns=const_cols, inplace=True)\n",
        "    print('dropped constant cols')\n",
        "\n",
        "print('final feature count:', X_train.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Simple baseline models\n",
        "Train a Dummy baseline, LogisticRegression and KNN to get baseline scores quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_model(model, X_tr, X_te, y_tr, y_te, name='model'):\n",
        "    model.fit(X_tr, y_tr)\n",
        "    y_pred = model.predict(X_te)\n",
        "    print('---', name)\n",
        "    print('Accuracy:', accuracy_score(y_te, y_pred))\n",
        "    print('Macro F1:', f1_score(y_te, y_pred, average='macro'))\n",
        "    print(classification_report(y_te, y_pred, zero_division=0))\n",
        "    cm = confusion_matrix(y_te, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=False, cmap='Blues')\n",
        "    plt.title(f'Confusion matrix: {name}')\n",
        "    plt.show()\n",
        "\n",
        "# Dummy baseline\n",
        "dummy = DummyClassifier(strategy='most_frequent')\n",
        "eval_model(dummy, X_train, X_test, y_train, y_test, name='Dummy (most frequent)')\n",
        "\n",
        "# Logistic with scaling\n",
        "pipe_lr = Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))])\n",
        "eval_model(pipe_lr, X_train, X_test, y_train, y_test, name='LogisticRegression')\n",
        "\n",
        "# KNN quick (baseline)\n",
        "pipe_knn = Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier(n_neighbors=5))])\n",
        "eval_model(pipe_knn, X_train, X_test, y_train, y_test, name='KNN (k=5)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Required models: RandomForest & XGBoost (quick training)\n",
        "Train default RandomForest and XGBoost as a first pass, then perform tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RandomForest quick\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
        "eval_model(rf, X_train, X_test, y_train, y_test, name='RandomForest (default-ish)')\n",
        "\n",
        "# XGBoost quick - using sklearn API\n",
        "n_classes = df['activity'].nunique()\n",
        "xgb_clf = xgb.XGBClassifier(objective='multi:softprob', num_class=n_classes,\n",
        "                            use_label_encoder=False, eval_metric='mlogloss', n_jobs=-1, random_state=RANDOM_STATE)\n",
        "eval_model(xgb_clf, X_train, X_test, y_train, y_test, name='XGBoost (default-ish)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) CatBoost (quick run and tuning template)\n",
        "CatBoost often performs well out of the box and handles categorical features — here we have only numeric features so we use default settings. If CatBoost not installed, install with `pip install catboost`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CatBoostClassifier is None:\n",
        "    print('CatBoost not installed. Install via `pip install catboost` to run this cell.')\n",
        "else:\n",
        "    cb = CatBoostClassifier(iterations=500, learning_rate=0.1, random_seed=RANDOM_STATE, verbose=0)\n",
        "    eval_model(cb, X_train, X_test, y_train, y_test, name='CatBoost (default-ish)')\n",
        "\n",
        "    # Tuning template (use RandomizedSearchCV or CatBoost's internal cv)\n",
        "    # Example with sklearn RandomizedSearchCV wrapper (can be slow):\n",
        "    # param_dist_cb = {\n",
        "    #     'depth': [4,6,8,10],\n",
        "    #     'learning_rate': [0.01, 0.05, 0.1],\n",
        "    #     'iterations': [200, 500, 800]\n",
        "    # }\n",
        "    # rnd_cb = RandomizedSearchCV(CatBoostClassifier(random_seed=RANDOM_STATE, verbose=0),\n",
        "    #                          param_distributions=param_dist_cb, n_iter=10, scoring='f1_macro', cv=StratifiedKFold(4), verbose=1)\n",
        "    # rnd_cb.fit(X_train, y_train)\n",
        "    # print('best params', rnd_cb.best_params_)\n",
        "    # best_cb = rnd_cb.best_estimator_\n",
        "    # eval_model(best_cb, X_train, X_test, y_train, y_test, name='CatBoost (tuned)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) KNN tuning (template)\n",
        "KNN requires scaling. Use GridSearchCV for k and distance weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_knn = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
        "param_grid_knn = {\n",
        "    'knn__n_neighbors': [3,5,7,9],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['minkowski', 'euclidean']\n",
        "}\n",
        "cv_knn = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_STATE)\n",
        "gs_knn = GridSearchCV(pipe_knn, param_grid_knn, scoring='f1_macro', cv=cv_knn, n_jobs=-1, verbose=1)\n",
        "\n",
        "# NOTE: uncomment to run full grid search\n",
        "# gs_knn.fit(X_train, y_train)\n",
        "# print('best knn', gs_knn.best_params_, gs_knn.best_score_)\n",
        "# best_knn = gs_knn.best_estimator_\n",
        "# eval_model(best_knn, X_train, X_test, y_train, y_test, name='KNN (tuned)')\n",
        "\n",
        "print('KNN tuning template ready. Uncomment the fit lines to run the search.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Example hyperparameter tuning for RandomForest (RandomizedSearchCV)\n",
        "Use StratifiedKFold and optimize macro-F1.\n",
        "This cell is a template — reduce n_iter or param ranges to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 400],\n",
        "    'max_depth': [None, 10, 20, 40],\n",
        "    'min_samples_split': [2, 4, 8],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "rnd = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1),\n",
        "                         param_distributions=param_dist, n_iter=12, scoring='f1_macro', cv=cv, verbose=1, random_state=RANDOM_STATE)\n",
        "\n",
        "# NOTE: this can take time. Uncomment to run.\n",
        "# rnd.fit(X_train, y_train)\n",
        "# print('best params', rnd.best_params_)\n",
        "# best_rf = rnd.best_estimator_\n",
        "# eval_model(best_rf, X_train, X_test, y_train, y_test, name='RandomForest (tuned)')\n",
        "\n",
        "print('RandomForest tuning template ready. Run the fit lines above when you want to tune.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) LightGBM quick run (optional)\n",
        "Left here as a comparison if you want to try it; not required since you have CatBoost, XGBoost and RF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lgb_clf = lgb.LGBMClassifier(objective='multiclass', num_class=n_classes, random_state=RANDOM_STATE)\n",
        "eval_model(lgb_clf, X_train, X_test, y_train, y_test, name='LightGBM (default)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Additional method: simple 1D-CNN on features (optional)\n",
        "If you prefer to keep experiments lighter, skip CNN. Kept here as optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, callbacks\n",
        "except Exception as e:\n",
        "    print('TensorFlow is not available in the environment:', e)\n",
        "    tf = None\n",
        "\n",
        "if tf is not None:\n",
        "    # simple data reshape for Conv1D: (samples, features, 1)\n",
        "    X_tr_cnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1)).astype('float32')\n",
        "    X_te_cnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1)).astype('float32')\n",
        "    y_tr_enc = tf.keras.utils.to_categorical(y_train - 1, num_classes=n_classes)  # labels 1..12 -> 0..11\n",
        "    y_te_enc = tf.keras.utils.to_categorical(y_test - 1, num_classes=n_classes)\n",
        "\n",
        "    def make_cnn(input_shape):\n",
        "        inp = layers.Input(shape=input_shape)\n",
        "        x = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(inp)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "        out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "        model = models.Model(inp, out)\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    cnn = make_cnn((X_tr_cnn.shape[1], 1))\n",
        "    cnn.summary()\n",
        "\n",
        "    es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    # NOTE: reduce epochs and batch_size for quick runs\n",
        "    history = cnn.fit(X_tr_cnn, y_tr_enc, validation_split=0.15, epochs=30, batch_size=64, callbacks=[es])\n",
        "\n",
        "    # evaluate\n",
        "    y_pred_proba = cnn.predict(X_te_cnn)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1) + 1\n",
        "    print('CNN results')\n",
        "    print('Accuracy', accuracy_score(y_test, y_pred))\n",
        "    print('Macro F1', f1_score(y_test, y_pred, average='macro'))\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "else:\n",
        "    print('TensorFlow not installed — skip CNN cell or install TF.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) SHAP and interpretation (template)\n",
        "Compute SHAP for tree models (RandomForest/XGBoost/CatBoost) to explain model behavior for classes and investigate feature importance for confusion cases.\n",
        "Be careful: SHAP can be slow on large feature sets or many samples. Use a sample subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import shap\n",
        "except Exception as e:\n",
        "    print('shap not available:', e)\n",
        "    shap = None\n",
        "\n",
        "# Use the best model from tuning above if available, else the quick rf/xgb/cb we trained\n",
        "model_for_shap = None\n",
        "if 'best_rf' in globals():\n",
        "    model_for_shap = globals()['best_rf']\n",
        "elif 'rf' in globals():\n",
        "    model_for_shap = rf\n",
        "elif 'xgb_clf' in globals():\n",
        "    model_for_shap = xgb_clf\n",
        "elif 'cb' in globals() and CatBoostClassifier is not None:\n",
        "    model_for_shap = cb\n",
        "\n",
        "if shap is not None and model_for_shap is not None:\n",
        "    # take a small sample to speed up\n",
        "    sample_idx = np.random.choice(X_test.shape[0], min(300, X_test.shape[0]), replace=False)\n",
        "    Xshap = X_test.iloc[sample_idx]\n",
        "\n",
        "    if hasattr(shap, 'TreeExplainer'):\n",
        "        explainer = shap.TreeExplainer(model_for_shap)\n",
        "        shap_values = explainer.shap_values(Xshap)\n",
        "        # summary_plot for multiclass will show one row per class\n",
        "        try:\n",
        "            shap.summary_plot(shap_values, Xshap)\n",
        "        except Exception as e:\n",
        "            print('plotting error:', e)\n",
        "    else:\n",
        "        print('No TreeExplainer available; consider KernelExplainer (slower).')\n",
        "else:\n",
        "    print('SHAP or a model for SHAP is not available. Install shap and/or run model training above.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) Ablation experiments (examples / templates)\n",
        "Below are templates for the non-trivial research questions in the assignment. Run each block to compare settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A) Time-domain vs Frequency-domain vs Combined\n",
        "time_cols = [c for c in X.columns if c.startswith('t')]\n",
        "freq_cols = [c for c in X.columns if c.startswith('f')]\n",
        "print('time_cols:', len(time_cols), 'freq_cols:', len(freq_cols))\n",
        "\n",
        "def train_and_report(cols, name):\n",
        "    Xt = X_train[cols]\n",
        "    Xv = X_test[cols]\n",
        "    rf_tmp = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
        "    rf_tmp.fit(Xt, y_train)\n",
        "    ypred = rf_tmp.predict(Xv)\n",
        "    print('\\n--', name)\n",
        "    print('Macro F1', f1_score(y_test, ypred, average='macro'))\n",
        "    return rf_tmp\n",
        "\n",
        "rf_time = None\n",
        "rf_freq = None\n",
        "if len(time_cols) > 0:\n",
        "    rf_time = train_and_report(time_cols, 'RandomForest on TIME features')\n",
        "if len(freq_cols) > 0:\n",
        "    rf_freq = train_and_report(freq_cols, 'RandomForest on FREQ features')\n",
        "if len(time_cols) > 0 and len(freq_cols) > 0:\n",
        "    rf_both = train_and_report(time_cols + freq_cols, 'RandomForest on COMBINED features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) Save models / results (examples)\n",
        "Save your best models and experiment results for the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "# Example: save rf if exists\n",
        "if 'best_rf' in globals():\n",
        "    joblib.dump(best_rf, 'models/best_rf.joblib')\n",
        "    print('saved best_rf')\n",
        "elif 'rf' in globals():\n",
        "    joblib.dump(rf, 'models/rf_quick.joblib')\n",
        "    print('saved rf_quick')\n",
        "\n",
        "if 'cb' in globals() and CatBoostClassifier is not None:\n",
        "    joblib.dump(cb, 'models/catboost_quick.joblib')\n",
        "    print('saved catboost_quick')\n",
        "\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17) Notes for the report\n",
        " - Include method descriptions and key hyperparameters tuned.\n",
        " - Present results (tables + confusion matrices) for all methods.\n",
        " - Answer at least two non-trivial questions (e.g., time vs frequency, accelerometer vs gyroscope contributions, ablation feature selection).\n",
        " - Use SHAP / feature importance to explain model decisions and show examples of misclassifications.\n",
        " - Add an explicit contributions section listing who implemented which notebook sections, experiments, and report parts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final: Next steps (suggested)\n",
        "1. Run model tuning for RF, XGBoost and CatBoost (use RandomizedSearchCV or CatBoost's own cv).\n",
        "2. Run KNN grid search for best k and distance options.\n",
        "3. Perform ablation experiments and SHAP analysis.\n",
        "4. Fill the report sections and clearly state contributions.\n",
        "\n",
        "If you want, I can generate a ready-to-run notebook with some moderate tuning ranges and tuned parameters filled in (this will still take time to run). Do you want me to prepare a version with tuning lines activated and moderate n_iter / param grids?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    },
    "title": "Human Activity Recognition - Course Project Notebook (RF, XGBoost, CatBoost, KNN)"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
